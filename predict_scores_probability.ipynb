{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Current State - \n",
    "# 5 days \n",
    "# MAX_DATE = '2020-01-01'\n",
    "# MIN_DATE = '2018-11-01'\n",
    "# With normalization\n",
    "\n",
    "# Probability 0.6826923076923077\n",
    "\n",
    "\n",
    "\n",
    "# ----------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_possessions_per_game_and_team(row,pbp):\n",
    "    try:\n",
    "        GAME_ID = row['GAME_ID']\n",
    "        is_home = row['IsHome']\n",
    "        single_game = pbp[pbp['GAME_ID']==GAME_ID]\n",
    "        index_indicator = 'Home_EOP' if is_home==1 else 'Visitor_EOP'\n",
    "\n",
    "        single_game = single_game.sort_values('EVENTNUM')\n",
    "        single_game['Shift_HOMEDESCRIPTION'] = single_game['HOMEDESCRIPTION'].shift(-1)\n",
    "        single_game[['Shift_HOMEDESCRIPTION','HOMEDESCRIPTION']]\n",
    "        single_game.loc[(single_game['HOMEDESCRIPTION'].isnull()==False)&\n",
    "                        (single_game['Shift_HOMEDESCRIPTION'].isnull()==True)&\n",
    "                        (single_game['EVENTMSGTYPE'].isin([1,2,3,4,5,7,9])),'Home_EOP'] = 1\n",
    "        single_game['Home_EOP'].fillna(0,inplace=True)\n",
    "        single_game[['Shift_HOMEDESCRIPTION','HOMEDESCRIPTION','VISITORDESCRIPTION','Home_EOP']]\n",
    "\n",
    "        single_game['Shift_VISITORDESCRIPTION'] = single_game['VISITORDESCRIPTION'].shift(-1)\n",
    "        single_game[['Shift_VISITORDESCRIPTION','VISITORDESCRIPTION']]\n",
    "        single_game.loc[(single_game['VISITORDESCRIPTION'].isnull()==False)&\n",
    "                        (single_game['Shift_VISITORDESCRIPTION'].isnull()==True)&\n",
    "                        (single_game['EVENTMSGTYPE'].isin([1,2,3,4,5,7,9])),'Visitor_EOP'] = 1\n",
    "        single_game['Visitor_EOP'].fillna(0,inplace=True)\n",
    "        return single_game[['Home_EOP','Visitor_EOP']].sum().loc[index_indicator]\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "import itertools\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def load_dfs( MAX_DATE = None , MIN_DATE = None):\n",
    "    nba_teams = ['MIL', 'CHI', 'CHA', 'TOR', 'BOS', 'PHX', 'OKC', 'LAC', 'IND',\n",
    "           'BKN', 'MIN', 'UTA', 'SAS', 'DAL', 'CLE', 'NYK', 'POR', 'HOU',\n",
    "           'DEN', 'MEM', 'SAC', 'PHI',  'ATL', 'LAL', \n",
    "           'WAS', 'ORL', 'GSW', 'NOP', \n",
    "           'MIA', \n",
    "           'DET']\n",
    "    nba_team_ids =[1610612749, 1610612766, 1610612738, 1610612746, 1610612754,\n",
    "           1610612750, 1610612741, 1610612742, 1610612762, 1610612759,\n",
    "           1610612739, 1610612752, 1610612761, 1610612760, 1610612757,\n",
    "           1610612751, 1610612745, 1610612756, 1610612743, 1610612755,\n",
    "           1610612737, 1610612763, 1610612764, 1610612744, 1610612740,\n",
    "           1610612753, 1610612747, 1610612758, 1610612748, 1610612765]\n",
    "\n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "    nba_teams_df = pd.DataFrame(nba_teams)\n",
    "    dummy_nba_teams= pd.get_dummies(nba_team_ids, prefix ='team')\n",
    "\n",
    "\n",
    "    df = pd.read_csv('large_boxscoretraditionalv2_df.csv')\n",
    "    df=df[df['TEAM_ID'].isin(nba_team_ids)]\n",
    "    df.drop_duplicates(subset = ['GAME_ID','PLAYER_ID'],inplace=True)\n",
    "    df.dropna(subset=['MIN'],inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "    df_g = pd.read_csv('all_games.csv')\n",
    "    df_g.drop_duplicates(subset =['TEAM_ID','GAME_ID'],inplace=True)\n",
    "\n",
    "    df_g.loc[df_g['WL']=='W','IsWin']=1\n",
    "    df_g.loc[df_g['WL']=='L','IsWin']=0\n",
    "    df_g['IsHome'] = (df_g['MATCHUP'].str.contains('vs.')).astype(int)\n",
    "    df_g['Opposing_team'] = df_g['MATCHUP'].apply( lambda x: x.split(' ')[2])\n",
    "\n",
    "    df_g=df_g[df_g['TEAM_ID'].isin(nba_team_ids)]\n",
    "    team_abv_to_id_mapping = df_g[['TEAM_ABBREVIATION','TEAM_ID']].drop_duplicates().set_index('TEAM_ABBREVIATION')['TEAM_ID'].to_dict()\n",
    "    df_g['Opposing_team_ID'] = df_g['Opposing_team'].apply(lambda x: team_abv_to_id_mapping[x] if x in team_abv_to_id_mapping else None )\n",
    "\n",
    "    df_g=df_g[df_g['Opposing_team_ID'].isin(nba_team_ids)]\n",
    "\n",
    "    # df_g=df_g[['GAME_ID','TEAM_ID','GAME_DATE']].drop_duplicates().dropna()\n",
    "    df_g['GAME_DATE'] = pd.to_datetime(df_g['GAME_DATE'])\n",
    "    \n",
    "    if MAX_DATE is not None:\n",
    "        df_g = df_g[df_g['GAME_DATE']<MAX_DATE]\n",
    "    if MIN_DATE is not None:\n",
    "        df_g = df_g[df_g['GAME_DATE']>MIN_DATE]\n",
    "    # add balance scoring\n",
    "\n",
    "\n",
    "    df = df.merge(df_g, \n",
    "         how='inner',\n",
    "         left_on=['GAME_ID','TEAM_ID'],\n",
    "         right_on=['GAME_ID','TEAM_ID'])\n",
    "\n",
    "\n",
    "    pbp = pd.read_csv('large_playbyplayv2_df.csv', index_col=0)\n",
    "    pbp.drop_duplicates(subset=['GAME_ID','EVENTNUM','PERIOD'], inplace=True)\n",
    "\n",
    "\n",
    "    shot_chart_df =  pd.read_csv('ShotChartDetail_v2.csv')\n",
    "    shot_chart_df.drop_duplicates(subset=['GAME_ID','GAME_EVENT_ID'], inplace=True)\n",
    "    \n",
    "    df_g['num_of_possessions'] = df_g.apply( lambda x: get_possessions_per_game_and_team(x,pbp), axis=1)\n",
    "    df_g.dropna(subset=['num_of_possessions'],inplace=True)\n",
    "    df_g['AdjustedPM'] = (df_g['PLUS_MINUS']/df_g['num_of_possessions'])*100\n",
    "    df_g['OffRating'] = (df_g['PTS']/df_g['num_of_possessions'])*100\n",
    "    df_g['EFG'] = (df_g['FGM'] + 0.5*df_g['FG3M'])/df_g['FGA']\n",
    "    df_g['AST_ratio'] = df_g['AST']*100/((df_g['FGA'])+(df_g['FTA']*0.44)+(df_g['AST'])+(df_g['AST']))\n",
    "    df_g['Opp_points'] = df_g['PTS'] + df_g['PLUS_MINUS']\n",
    "    df_g['Def_Rating'] = (df_g['Opp_points'] / df_g['num_of_possessions'])*100\n",
    "\n",
    "    ['AdjustedPM','OffRating','EFG','AST_ratio','Opp_points','Def_Rating']\n",
    "\n",
    "\n",
    "\n",
    "    print('done')\n",
    "    return df,df_g,pbp,shot_chart_df,team_abv_to_id_mapping\n",
    "\n",
    "df,df_g,pbp,shot_chart_df,team_abv_to_id_mapping = load_dfs(  MAX_DATE = '2020-02-10',MIN_DATE = '2018-11-01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nba_teams = ['MIL', 'CHI', 'CHA', 'TOR', 'BOS', 'PHX', 'OKC', 'LAC', 'IND',\n",
    "       'BKN', 'MIN', 'UTA', 'SAS', 'DAL', 'CLE', 'NYK', 'POR', 'HOU',\n",
    "       'DEN', 'MEM', 'SAC', 'PHI',  'ATL', 'LAL', \n",
    "       'WAS', 'ORL', 'GSW', 'NOP', \n",
    "       'MIA', \n",
    "       'DET']\n",
    "nba_team_ids =[1610612749, 1610612766, 1610612738, 1610612746, 1610612754,\n",
    "       1610612750, 1610612741, 1610612742, 1610612762, 1610612759,\n",
    "       1610612739, 1610612752, 1610612761, 1610612760, 1610612757,\n",
    "       1610612751, 1610612745, 1610612756, 1610612743, 1610612755,\n",
    "       1610612737, 1610612763, 1610612764, 1610612744, 1610612740,\n",
    "       1610612753, 1610612747, 1610612758, 1610612748, 1610612765]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "nba_teams_df = pd.DataFrame(nba_teams)\n",
    "dummy_nba_teams= pd.get_dummies(nba_team_ids, prefix ='team')\n",
    "\n",
    "sliding_window_num_of_games = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1610612749\n",
      "1610612766\n",
      "1610612738\n",
      "1610612746\n",
      "1610612754\n",
      "1610612750\n",
      "1610612741\n",
      "1610612742\n",
      "1610612762\n",
      "1610612759\n",
      "1610612739\n",
      "1610612752\n",
      "1610612761\n",
      "1610612760\n",
      "1610612757\n",
      "1610612751\n",
      "1610612745\n",
      "1610612756\n",
      "1610612743\n",
      "1610612755\n",
      "1610612737\n",
      "1610612763\n",
      "1610612764\n",
      "1610612744\n",
      "1610612740\n",
      "1610612753\n",
      "1610612747\n",
      "1610612758\n",
      "1610612748\n",
      "1610612765\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "large_df_of_sequences = pd.DataFrame([])\n",
    " \n",
    "# create the sliding window dataframe per team\n",
    "for team in nba_team_ids:\n",
    "    print(team)\n",
    "    x = df_g[df_g['TEAM_ID'] == team].sort_values('GAME_DATE').reset_index(drop=True)\n",
    "    \n",
    "    \n",
    "    dummy_curr_team = pd.get_dummies(x['TEAM_ID'], prefix ='team').reindex(columns = dummy_nba_teams.columns, fill_value=0)\n",
    "    dummy_curr_team.columns = ['curr_' + col for col in dummy_curr_team.columns]\n",
    "    team_indicator = dummy_curr_team.loc[0].values\n",
    "    \n",
    "    \n",
    "    list_team_indicator = list(team_indicator) \n",
    "    for ix,row in x[sliding_window_num_of_games:].iterrows():\n",
    "        tmp = x.loc[ix-sliding_window_num_of_games:ix]\n",
    "        \n",
    "        # add sequence of wins\n",
    "        list_of_sequence_of_wins = list(tmp['IsWin'].values)\n",
    "        new_cols = []\n",
    "        for game in range(sliding_window_num_of_games):\n",
    "                new_cols.insert(0,'{}_games_back_IsWin'.format(game + 1))\n",
    "        new_cols.append('Y') \n",
    "        df_of_sequence_of_wins = pd.DataFrame([list_of_sequence_of_wins],columns=new_cols)\n",
    "        \n",
    "        # get team indicator\n",
    "        team_indicator = dummy_curr_team[0:1]\n",
    "        \n",
    "        # is at home\n",
    "        list_of_sequence_of_at_home = list(tmp['IsHome'].values)\n",
    "        new_cols = []\n",
    "        for game in range(sliding_window_num_of_games+1):\n",
    "                new_cols.insert(0,'{}_games_back_IsHome'.format(game + 1))\n",
    "        df_of_sequence_of_at_home = pd.DataFrame([list_of_sequence_of_at_home],columns=new_cols)\n",
    "        \n",
    "        # get opp team indicator \n",
    "        opp_dummy_teams = pd.get_dummies(tmp['Opposing_team_ID'].astype(int), prefix ='team').reindex(columns = dummy_nba_teams.columns, fill_value=0)\n",
    "        opp_dummy_teams.columns = ['opp_' + col for col in opp_dummy_teams.columns]\n",
    "        appended_list_of_encoded_opp_teams = list(itertools.chain(*opp_dummy_teams.values))\n",
    "        appended_df_of_encoded_opp_teams = pd.DataFrame([appended_list_of_encoded_opp_teams])\n",
    "        \n",
    "        # add averages of categories\n",
    "        cols_to_calc = ['PTS','REB','AST','STL','BLK','TOV','PLUS_MINUS','FG3_PCT','FG_PCT','FT_PCT','num_of_possessions',\n",
    "                       'AdjustedPM','OffRating','EFG','AST_ratio','Opp_points','Def_Rating']\n",
    "        averages_of_categories_curr_team = tmp[cols_to_calc][:-1].mean().values\n",
    "        df_of_averages_of_categories_curr_team = pd.DataFrame([averages_of_categories_curr_team],columns =cols_to_calc)\n",
    "        \n",
    "        \n",
    "        # add shot area avgs for curr team\n",
    "        shot_index= ['Mid-Range', 'In The Paint (Non-RA)', 'Restricted Area',\n",
    "       'Above the Break 3', 'Left Corner 3', 'Right Corner 3']\n",
    "\n",
    "        shot_chart_df_for_tmp = shot_chart_df[(shot_chart_df['GAME_ID'].isin(tmp[:-1]['GAME_ID'].values))&\n",
    "                                 (shot_chart_df['TEAM_ID']==team)].groupby(\n",
    "                                    ['SHOT_ZONE_BASIC']).mean()['SHOT_MADE_FLAG'].reset_index()\n",
    "        shot_chart_df_for_tmp_reindex = shot_chart_df_for_tmp[['SHOT_ZONE_BASIC','SHOT_MADE_FLAG']].set_index('SHOT_ZONE_BASIC').reindex(shot_index).fillna(0)\n",
    "        shot_area_avgs = shot_chart_df_for_tmp_reindex.T.reset_index(drop=True)\n",
    "        \n",
    "        # add shot chart of opposing teams during window\n",
    "        shot_chart_df_for_tmp = shot_chart_df[(shot_chart_df['GAME_ID'].isin(tmp[:-1]['GAME_ID'].values))&\n",
    "                                 (shot_chart_df['TEAM_ID']!=team)].groupby(\n",
    "                                    ['SHOT_ZONE_BASIC']).mean()['SHOT_MADE_FLAG'].reset_index()\n",
    "        shot_chart_df_for_tmp_reindex = shot_chart_df_for_tmp[['SHOT_ZONE_BASIC','SHOT_MADE_FLAG']].set_index('SHOT_ZONE_BASIC').reindex(shot_index).fillna(0)\n",
    "        shot_area_avgs_opp = shot_chart_df_for_tmp_reindex.T.reset_index(drop=True)\n",
    "        \n",
    "         # add averages of categories for past opponents\n",
    "        list_of_opp_ids = list(tmp['Opposing_team_ID'].unique())\n",
    "\n",
    "        x_opp = df_g[(df_g['TEAM_ID'].isin(list_of_opp_ids))&\n",
    "                    (df_g['GAME_DATE'] < tmp['GAME_DATE'].max())&\n",
    "                    (df_g['GAME_DATE'] >= tmp['GAME_DATE'].min())].sort_values('GAME_DATE').reset_index(drop=True)\n",
    "        tmp_opp = x_opp[-sliding_window_num_of_games:ix]\n",
    "        cols_to_calc = ['PTS','REB','AST','STL','BLK','TOV','PLUS_MINUS','FG3_PCT','FG_PCT','FT_PCT','num_of_possessions',\n",
    "                       'AdjustedPM','OffRating','EFG','AST_ratio','Opp_points','Def_Rating']\n",
    "        averages_of_categories_opp_team = tmp_opp[cols_to_calc].mean().values\n",
    "        df_of_averages_of_categories_opp_team = pd.DataFrame([averages_of_categories_opp_team],columns = cols_to_calc).reset_index(drop=True)\n",
    "        \n",
    "        # add sequence of days between games\n",
    "        tmp['GAME_DATE'].diff().dt.days.dropna()\n",
    "        l = tmp['GAME_DATE'].diff().dt.days.dropna().values\n",
    "        l = [4 if sl >4 else sl for sl in l]\n",
    "        new_cols = []\n",
    "        for game in range(sliding_window_num_of_games):\n",
    "                new_cols.insert(0,'{}_games_back_PTS'.format(game + 1))\n",
    "\n",
    "        df_days_between = pd.DataFrame([l],columns=new_cols)\n",
    "\n",
    "        # start merging all together \n",
    "        # 1. wins\n",
    "        # 2. team indicator\n",
    "        # 3. at home indicator\n",
    "        # 4. TOV sequence\n",
    "#         print(len(team_indicator.columns),'team_indicator')\n",
    "#         print(len(df_of_sequence_of_wins.columns),'df_of_sequence_of_wins')\n",
    "#         print(len(df_of_sequence_of_at_home.columns),'df_of_sequence_of_at_home')\n",
    "#         print(len(appended_df_of_encoded_opp_teams.columns),'appended_df_of_encoded_opp_teams')\n",
    "#         print(len(df_of_averages_of_categories_curr_team.columns),'df_of_averages_of_categories_curr_team')\n",
    "#         print(len(shot_area_avgs.columns),'shot_area_avgs')\n",
    "#         print(len(shot_area_avgs_opp.columns),'shot_area_avgs_opp')\n",
    "        \n",
    "        df_all_features = team_indicator.merge(df_of_sequence_of_wins , how='inner' , left_index=True , right_index=True)\n",
    "        df_all_features = df_all_features.merge(df_of_sequence_of_at_home, how='inner' , left_index=True , right_index=True)\n",
    "        df_all_features = df_all_features.merge(appended_df_of_encoded_opp_teams, how='inner' , left_index=True , right_index=True)\n",
    "        df_all_features = df_all_features.merge(df_of_averages_of_categories_curr_team, how='inner' , left_index=True , right_index=True)\n",
    "        df_all_features = df_all_features.merge(shot_area_avgs, how='inner' , left_index=True , right_index=True)\n",
    "        df_all_features = df_all_features.merge(shot_area_avgs_opp, how='inner' , left_index=True , right_index=True)\n",
    "        df_all_features = df_all_features.merge(df_of_averages_of_categories_opp_team, how='inner' , left_index=True , right_index=True)\n",
    "        df_all_features = df_all_features.merge(df_days_between, how='inner' , left_index=True , right_index=True)\n",
    "\n",
    "        large_df_of_sequences = large_df_of_sequences.append(df_all_features)\n",
    "    \n",
    "large_df_of_sequences.dropna(inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6361323155216285\n",
      "[[252 166]\n",
      " [120 248]]\n",
      "LR: 0.597246 (0.025269)\n",
      "SVM: 0.614764 (0.029703)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "\n",
    "X = large_df_of_sequences.loc[:, large_df_of_sequences.columns != 'Y']\n",
    "saved_columns = X.columns\n",
    "x = X.values #returns a numpy array\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "x_scaled = min_max_scaler.fit_transform(x)\n",
    "X = pd.DataFrame(x_scaled, columns=saved_columns)\n",
    "\n",
    "\n",
    "\n",
    "Y = large_df_of_sequences['Y']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.25, random_state=10)\n",
    "\n",
    "\n",
    "logisticRegr = LogisticRegression()\n",
    "logisticRegr.fit(x_train, y_train)\n",
    "\n",
    "score = logisticRegr.score(x_test, y_test)\n",
    "print(score)\n",
    "predictions = logisticRegr.predict(x_test)\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import metrics\n",
    "cm = metrics.confusion_matrix(y_test, predictions)\n",
    "print(cm)\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt   \n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from skmultilearn.problem_transform import BinaryRelevance\n",
    "from skmultilearn.problem_transform import ClassifierChain\n",
    "from skmultilearn.problem_transform import LabelPowerset\n",
    "from skmultilearn.adapt import MLkNN\n",
    "models = []\n",
    "models.append(('LR', LogisticRegression()))\n",
    "# models.append(('KNN', KNeighborsClassifier()))\n",
    "# models.append(('NB', MultinomialNB()))\n",
    "models.append(('SVM', SVC()))\n",
    "\n",
    "\n",
    "seed = 2\n",
    "\n",
    "# evaluate each model in turn\n",
    "results = []\n",
    "names = []\n",
    "scoring = 'accuracy'\n",
    "for name, model in models:\n",
    "    kfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "    cv_results = model_selection.cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std() )\n",
    "    print(msg)\n",
    "# boxplot algorithm comparison\n",
    "fig = plt.figure()\n",
    "fig.suptitle('Algorithm Comparison')\n",
    "ax = fig.add_subplot(111)\n",
    "plt.boxplot(results)\n",
    "ax.set_xticklabels(names)\n",
    "plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 120 candidates, totalling 600 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:  2.9min\n",
      "[Parallel(n_jobs=-1)]: Done 426 tasks      | elapsed:  5.9min\n",
      "[Parallel(n_jobs=-1)]: Done 600 out of 600 | elapsed:  9.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set found on development set:\n",
      "{'classifier': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "                       max_depth=None, max_features=40, max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=130,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False), 'classifier__max_features': 40, 'classifier__n_estimators': 130}\n",
      "Detailed classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.65      0.57      0.61       418\n",
      "         1.0       0.57      0.64      0.60       368\n",
      "\n",
      "    accuracy                           0.61       786\n",
      "   macro avg       0.61      0.61      0.61       786\n",
      "weighted avg       0.61      0.61      0.61       786\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "\n",
    "X = large_df_of_sequences.loc[:, large_df_of_sequences.columns != 'Y']\n",
    "saved_columns = X.columns\n",
    "x = X.values #returns a numpy array\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "x_scaled = min_max_scaler.fit_transform(x)\n",
    "X = pd.DataFrame(x_scaled, columns=saved_columns)\n",
    "\n",
    "\n",
    "Y = large_df_of_sequences['Y']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.25, random_state=10)\n",
    "\n",
    "\n",
    "\n",
    "pipe = Pipeline([('classifier' , RandomForestClassifier())])\n",
    "# pipe = Pipeline([('classifier', RandomForestClassifier())])\n",
    "\n",
    "# Create param grid.\n",
    "\n",
    "param_grid = [\n",
    "#     {'classifier' : [LogisticRegression()],\n",
    "#      'classifier__penalty' : ['l1', 'l2'],\n",
    "#     'classifier__C' : np.logspace(-4, 4, 20),\n",
    "#     'classifier__solver' : ['liblinear']}\n",
    "#     ,\n",
    "    {'classifier' : [RandomForestClassifier()],\n",
    "    'classifier__n_estimators' : list(range(50,150,10)),\n",
    "    'classifier__max_features' : list(range(30,150,10))}\n",
    "]\n",
    "\n",
    "# Create grid search object\n",
    "\n",
    "clf = GridSearchCV(pipe, param_grid = param_grid, cv = 5, verbose=True, n_jobs=-1)\n",
    "\n",
    "# Fit on data\n",
    "\n",
    "best_clf = clf.fit(x_train, y_train)\n",
    "from sklearn.metrics import classification_report\n",
    "print(\"Best parameters set found on development set:\")\n",
    "print(best_clf.best_params_)\n",
    "print(\"Detailed classification report:\")\n",
    "y_true, y_pred = y_test, best_clf.predict(x_test)\n",
    "print(classification_report(y_true, y_pred))\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "filename_probs = 'Probabilities_2018-11-01_2020-02-04_5Days.sav'\n",
    "pickle.dump(best_clf, open(filename_probs,'wb'))\n",
    "best_clf = pickle.load(open(filename_probs, 'rb'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "2020-02-12T00:00:00.000000000\n",
      "done\n",
      "2020-02-11 00:00:00\n",
      "['DEN', 1610612743, 'LAL', 1610612747, 0.7307692307692307]\n",
      "\n",
      "['ORL', 1610612753, 'DET', 1610612765, 0.38461538461538464]\n",
      "\n",
      "['CLE', 1610612739, 'ATL', 1610612737, 0.3230769230769231]\n",
      "\n",
      "['UTA', 1610612762, 'MIA', 1610612748, 0.4461538461538462]\n",
      "\n",
      "['NYK', 1610612752, 'WAS', 1610612764, 0.5307692307692308]\n",
      "\n",
      "['MIN', 1610612750, 'CHA', 1610612766, 0.45384615384615384]\n",
      "\n",
      "['BKN', 1610612751, 'TOR', 1610612761, 0.49230769230769234]\n",
      "\n",
      "['MEM', 1610612763, 'POR', 1610612757, 0.47692307692307695]\n",
      "\n",
      "['DAL', 1610612742, 'SAC', 1610612758, 0.5769230769230769]\n",
      "\n",
      "['PHX', 1610612756, 'GSW', 1610612744, 0.4153846153846154]\n",
      "\n",
      "['IND', 1610612754, 'MIL', 1610612749, 0.2692307692307692]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# lsit_of_games = [\n",
    "#                 ['BOS','POR'] , \n",
    "#                 ['CLE','DET'] , \n",
    "#                 ['BKN','OKC'] , \n",
    "#                 ['MEM','MIN'] , \n",
    "#                 ['PHX','SAC'] , \n",
    "#                 ['LAL','NYK'] , \n",
    "                \n",
    "#             ]\n",
    "\n",
    "# df,df_g,pbp,shot_chart_df,team_abv_to_id_mapping = load_dfs( MAX_DATE = '2020-01-07')\n",
    "total_results_saved = []\n",
    "\n",
    "\n",
    "\n",
    "FIRST_DATE_TO_CHECK = '2020-02-11'\n",
    "LAST_DATE_TO_RUN_ON = '2020-02-13'\n",
    "\n",
    "df,df_g_global ,pbp,shot_chart_df,team_abv_to_id_mapping = load_dfs( MAX_DATE = LAST_DATE_TO_RUN_ON)\n",
    "\n",
    "list_of_games_to_check = df_g_global[['GAME_DATE','TEAM_ABBREVIATION','GAME_ID','IsHome']]\n",
    "len(list_of_games_to_check)\n",
    "merged_list_of_games_to_check = list_of_games_to_check.merge(list_of_games_to_check,\n",
    "                                                             how='inner',\n",
    "                                                             left_on =['GAME_ID'] ,\n",
    "                                                             right_on =['GAME_ID'])\n",
    "merged_list_of_games_to_check = merged_list_of_games_to_check[merged_list_of_games_to_check['TEAM_ABBREVIATION_x']!= merged_list_of_games_to_check['TEAM_ABBREVIATION_y']]\n",
    "merged_list_of_games_to_check=merged_list_of_games_to_check[merged_list_of_games_to_check['IsHome_x']==1]\n",
    "\n",
    "\n",
    "merged_list_of_games_to_check = merged_list_of_games_to_check[merged_list_of_games_to_check['GAME_DATE_x']>FIRST_DATE_TO_CHECK]\n",
    "list_of_dates = merged_list_of_games_to_check['GAME_DATE_x'].unique()\n",
    "for single_date in list_of_dates:\n",
    "    print (single_date)\n",
    "    lsit_of_games = []\n",
    "    for ix,row in merged_list_of_games_to_check[merged_list_of_games_to_check['GAME_DATE_x']==single_date].iterrows():\n",
    "        game=[row['TEAM_ABBREVIATION_x'],row['TEAM_ABBREVIATION_y']]\n",
    "        lsit_of_games.append(game)\n",
    "    df,df_g,pbp,shot_chart_df,team_abv_to_id_mapping = load_dfs( MAX_DATE = str(single_date)[:10])\n",
    "           \n",
    "        \n",
    "        \n",
    "        \n",
    "    print(df_g['GAME_DATE'].max())\n",
    "    for game in lsit_of_games:\n",
    "    #     print(game)\n",
    "\n",
    "        df_g['TEAM_ABBREVIATION'].unique()\n",
    "        ###########################\n",
    "        ### DEFINE YOUR TEAMS   ###\n",
    "        ###########################\n",
    "        curr_team_abv = game[0] #'PHX'\n",
    "        curr_team_id = team_abv_to_id_mapping[curr_team_abv]\n",
    "\n",
    "        opp_team = game[1] #'SAC'\n",
    "        opp_team_id  = team_abv_to_id_mapping[opp_team]\n",
    "        is_current_game_at_home = 1\n",
    "\n",
    "\n",
    "\n",
    "        ###########################\n",
    "\n",
    "        x = df_g[df_g['TEAM_ID'] == curr_team_id].sort_values('GAME_DATE').reset_index(drop=True)\n",
    "    #     print(tmp)\n",
    "        tmp = x[-sliding_window_num_of_games:]\n",
    "\n",
    "        dummy_curr_team = pd.get_dummies(x['TEAM_ID'].astype(int), prefix ='team').reindex(columns = dummy_nba_teams.columns, fill_value=0)\n",
    "        dummy_curr_team.columns = ['curr_' + col for col in dummy_curr_team.columns]\n",
    "        team_indicator = dummy_curr_team.loc[0].values\n",
    "\n",
    "        list_of_sequence_of_wins = list(tmp['IsWin'].values)\n",
    "        new_cols = []\n",
    "        for game in range(sliding_window_num_of_games):\n",
    "                new_cols.insert(0,'{}_games_back_IsWin'.format(game + 1))\n",
    "        df_of_sequence_of_wins = pd.DataFrame([list_of_sequence_of_wins],columns=new_cols)\n",
    "\n",
    "        # # get team indicator\n",
    "        team_indicator = dummy_curr_team[0:1]\n",
    "        team_indicator\n",
    "\n",
    "        # is at home\n",
    "        list_of_sequence_of_at_home = list(tmp['IsHome'].values)\n",
    "        new_cols = []\n",
    "        for game in range(sliding_window_num_of_games):\n",
    "                new_cols.insert(0,'{}_games_back_IsHome'.format(game + 1))\n",
    "        df_of_sequence_of_at_home = pd.DataFrame([list_of_sequence_of_at_home],columns=new_cols)\n",
    "        df_of_sequence_of_at_home['{}_games_back_IsHome'.format(sliding_window_num_of_games+1)] = is_current_game_at_home\n",
    "\n",
    "\n",
    "        # get opp team indicator \n",
    "\n",
    "        opp_dummy_teams = pd.get_dummies(tmp['Opposing_team_ID'].astype(int).append(pd.Series(opp_team_id)), prefix ='team').reindex(columns = dummy_nba_teams.columns, fill_value=0)\n",
    "        opp_dummy_teams.columns = ['opp_' + col for col in opp_dummy_teams.columns]\n",
    "        appended_list_of_encoded_opp_teams = list(itertools.chain(*opp_dummy_teams.values))\n",
    "        appended_df_of_encoded_opp_teams = pd.DataFrame([appended_list_of_encoded_opp_teams])\n",
    "\n",
    "        # add averages of categories\n",
    "        cols_to_calc = ['PTS','REB','AST','STL','BLK','TOV','PLUS_MINUS','FG3_PCT','FG_PCT','FT_PCT','num_of_possessions',\n",
    "                           'AdjustedPM','OffRating','EFG','AST_ratio','Opp_points','Def_Rating']\n",
    "        averages_of_categories_curr_team = tmp[cols_to_calc].mean().values\n",
    "        df_of_averages_of_categories_curr_team = pd.DataFrame([averages_of_categories_curr_team],columns=cols_to_calc)\n",
    "        df_of_averages_of_categories_curr_team\n",
    "\n",
    "        # add shot area avgs for curr team\n",
    "        shot_index= ['Mid-Range', 'In The Paint (Non-RA)', 'Restricted Area',\n",
    "        'Above the Break 3', 'Left Corner 3', 'Right Corner 3']\n",
    "\n",
    "        shot_chart_df_for_tmp = shot_chart_df[(shot_chart_df['GAME_ID'].isin(tmp['GAME_ID'].values))&\n",
    "                                 (shot_chart_df['TEAM_ID']==curr_team_id)].groupby(\n",
    "                                    ['SHOT_ZONE_BASIC']).mean()['SHOT_MADE_FLAG'].reset_index()\n",
    "        shot_chart_df_for_tmp_reindex = shot_chart_df_for_tmp[['SHOT_ZONE_BASIC','SHOT_MADE_FLAG']].set_index('SHOT_ZONE_BASIC').reindex(shot_index).fillna(0)\n",
    "        shot_area_avgs = shot_chart_df_for_tmp_reindex.T.reset_index(drop=True)\n",
    "        shot_area_avgs\n",
    "\n",
    "        # add shot chart of opposing teams during window\n",
    "        shot_chart_df_for_tmp = shot_chart_df[(shot_chart_df['GAME_ID'].isin(tmp[:-1]['GAME_ID'].values))&\n",
    "                                 (shot_chart_df['TEAM_ID']!=curr_team_id)].groupby(\n",
    "                                    ['SHOT_ZONE_BASIC']).mean()['SHOT_MADE_FLAG'].reset_index()\n",
    "        shot_chart_df_for_tmp_reindex = shot_chart_df_for_tmp[['SHOT_ZONE_BASIC','SHOT_MADE_FLAG']].set_index('SHOT_ZONE_BASIC').reindex(shot_index).fillna(0)\n",
    "        shot_area_avgs_opp = shot_chart_df_for_tmp_reindex.T.reset_index(drop=True)\n",
    "        shot_area_avgs_opp\n",
    "\n",
    "          # add averages of categories for past opponents\n",
    "        list_of_opp_ids = list(tmp['Opposing_team_ID'].unique())\n",
    "\n",
    "        x_opp = df_g[(df_g['TEAM_ID'].isin(list_of_opp_ids))&\n",
    "                    (df_g['GAME_DATE'] < tmp['GAME_DATE'].max())&\n",
    "                    (df_g['GAME_DATE'] >= tmp['GAME_DATE'].min())].sort_values('GAME_DATE').reset_index(drop=True)\n",
    "        tmp_opp = x_opp[-sliding_window_num_of_games:ix]\n",
    "        cols_to_calc = ['PTS','REB','AST','STL','BLK','TOV','PLUS_MINUS','FG3_PCT','FG_PCT','FT_PCT','num_of_possessions',\n",
    "                       'AdjustedPM','OffRating','EFG','AST_ratio','Opp_points','Def_Rating']\n",
    "        averages_of_categories_opp_team = tmp_opp[cols_to_calc].mean().values\n",
    "        df_of_averages_of_categories_opp_team = pd.DataFrame([averages_of_categories_opp_team],columns = cols_to_calc).reset_index(drop=True)\n",
    "\n",
    "        # add sequence of days between games\n",
    "        tmp_days_back = x[-sliding_window_num_of_games-1:]\n",
    "        l = tmp_days_back['GAME_DATE'].diff().dt.days.dropna().values\n",
    "        l = [4 if sl >4 else sl for sl in l]\n",
    "        new_cols = []\n",
    "        for game in range(sliding_window_num_of_games):\n",
    "                new_cols.insert(0,'{}_games_back_PTS'.format(game + 1))\n",
    "\n",
    "        df_days_between = pd.DataFrame([l],columns=new_cols)\n",
    "\n",
    "        # start merging all together \n",
    "        # 1. wins\n",
    "        # 2. team indicator\n",
    "        # 3. at home indicator\n",
    "        # 4. TOV sequence\n",
    "        # print(len(team_indicator.columns),'team_indicator')\n",
    "        # print(len(df_of_sequence_of_wins.columns),'df_of_sequence_of_wins')\n",
    "        # print(len(df_of_sequence_of_at_home.columns),'df_of_sequence_of_at_home')\n",
    "        # print(len(appended_df_of_encoded_opp_teams.columns),'appended_df_of_encoded_opp_teams')\n",
    "        # print(len(df_of_averages_of_categories_curr_team.columns),'df_of_averages_of_categories_curr_team')\n",
    "        # print(len(shot_area_avgs.columns),'shot_area_avgs')\n",
    "        # print(len(shot_area_avgs_opp.columns),'shot_area_avgs_opp')\n",
    "        df_all_features = team_indicator.merge(df_of_sequence_of_wins , how='inner' , left_index=True , right_index=True)\n",
    "        df_all_features = df_all_features.merge(df_of_sequence_of_at_home, how='inner' , left_index=True , right_index=True)\n",
    "        df_all_features = df_all_features.merge(appended_df_of_encoded_opp_teams, how='inner' , left_index=True , right_index=True)\n",
    "        df_all_features = df_all_features.merge(df_of_averages_of_categories_curr_team, how='inner' , left_index=True , right_index=True)\n",
    "        df_all_features = df_all_features.merge(shot_area_avgs, how='inner' , left_index=True , right_index=True)\n",
    "        df_all_features = df_all_features.merge(shot_area_avgs_opp, how='inner' , left_index=True , right_index=True)\n",
    "        df_all_features = df_all_features.merge(df_of_averages_of_categories_opp_team, how='inner' , left_index=True , right_index=True)\n",
    "        df_all_features = df_all_features.merge(df_days_between, how='inner' , left_index=True , right_index=True)\n",
    "        \n",
    "        \n",
    "        df_all_features = df_all_features[X.columns]\n",
    "#         df_all_features = df_all_features.loc[:, df_all_features.columns != 'Y']\n",
    "        df_all_features = pd.DataFrame(min_max_scaler.transform(df_all_features),columns=df_all_features.columns)\n",
    "    \n",
    "    #     print(df_all_features.columns)\n",
    "        len(X.columns)\n",
    "        \n",
    "        print([curr_team_abv,curr_team_id, opp_team, opp_team_id, best_clf.predict_proba(df_all_features)[0][1]])\n",
    "        print()\n",
    "        results_saved = [single_date,curr_team_abv,curr_team_id, opp_team, opp_team_id, best_clf.predict_proba(df_all_features)[0][1]]\n",
    "        total_results_saved.append(results_saved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(total_results_saved,\n",
    "            columns=['Date','HomeTeam','HomeTeamID','AwayTeam','AwayTeamID','Probability']).to_csv('~/probs_Jan.csv',index=False, mode='a', header=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "2020-02-11 00:00:00\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Number of features of the model must match the input. Model n_features is 272 and input n_features is 267 ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-b84d27544bb1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    126\u001b[0m     \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcurr_team_abv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcurr_team_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopp_team\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopp_team_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbest_clf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_all_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\metaestimators.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m         \u001b[1;31m# lambda, but not partial, allows help() to work with update_wrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 116\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    117\u001b[0m         \u001b[1;31m# update the docstring of the returned function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m         \u001b[0mupdate_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    472\u001b[0m         \"\"\"\n\u001b[0;32m    473\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'predict_proba'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 474\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    475\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    476\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mif_delegate_has_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdelegate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'best_estimator_'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'estimator'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\metaestimators.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m         \u001b[1;31m# lambda, but not partial, allows help() to work with update_wrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 116\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    117\u001b[0m         \u001b[1;31m# update the docstring of the returned function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m         \u001b[0mupdate_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    472\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransform\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwith_final\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    473\u001b[0m             \u001b[0mXt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 474\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    475\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    476\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mif_delegate_has_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdelegate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'_final_estimator'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    586\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'estimators_'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    587\u001b[0m         \u001b[1;31m# Check data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 588\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_X_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    589\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m         \u001b[1;31m# Assign chunk of trees to jobs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py\u001b[0m in \u001b[0;36m_validate_X_predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    357\u001b[0m                                  \"call `fit` before exploiting the model.\")\n\u001b[0;32m    358\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 359\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimators_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_X_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    360\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    361\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\tree\\tree.py\u001b[0m in \u001b[0;36m_validate_X_predict\u001b[1;34m(self, X, check_input)\u001b[0m\n\u001b[0;32m    400\u001b[0m                              \u001b[1;34m\"match the input. Model n_features is %s and \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    401\u001b[0m                              \u001b[1;34m\"input n_features is %s \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 402\u001b[1;33m                              % (self.n_features_, n_features))\n\u001b[0m\u001b[0;32m    403\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    404\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Number of features of the model must match the input. Model n_features is 272 and input n_features is 267 "
     ]
    }
   ],
   "source": [
    "################\n",
    "# RUN PREDICTED\n",
    "################\n",
    "LAST_DATE_TO_RUN_ON = '2020-02-13'\n",
    "\n",
    "df,df_g_global ,pbp,shot_chart_df,team_abv_to_id_mapping = load_dfs( MAX_DATE = LAST_DATE_TO_RUN_ON)\n",
    "           \n",
    "lsit_of_games = [\n",
    "['BOS','LAC'],\n",
    "['NOP','OKC'],\n",
    "]\n",
    "\n",
    "print(df_g['GAME_DATE'].max())\n",
    "for game in lsit_of_games:\n",
    "#     print(game)\n",
    "\n",
    "    df_g['TEAM_ABBREVIATION'].unique()\n",
    "    ###########################\n",
    "    ### DEFINE YOUR TEAMS   ###\n",
    "    ###########################\n",
    "    curr_team_abv = game[0] #'PHX'\n",
    "    curr_team_id = team_abv_to_id_mapping[curr_team_abv]\n",
    "\n",
    "    opp_team = game[1] #'SAC'\n",
    "    opp_team_id  = team_abv_to_id_mapping[opp_team]\n",
    "    is_current_game_at_home = 1\n",
    "\n",
    "\n",
    "\n",
    "    ###########################\n",
    "\n",
    "    x = df_g[df_g['TEAM_ID'] == curr_team_id].sort_values('GAME_DATE').reset_index(drop=True)\n",
    "#     print(tmp)\n",
    "    tmp = x[-sliding_window_num_of_games:]\n",
    "\n",
    "    dummy_curr_team = pd.get_dummies(x['TEAM_ID'].astype(int), prefix ='team').reindex(columns = dummy_nba_teams.columns, fill_value=0)\n",
    "    dummy_curr_team.columns = ['curr_' + col for col in dummy_curr_team.columns]\n",
    "    team_indicator = dummy_curr_team.loc[0].values\n",
    "\n",
    "    list_of_sequence_of_wins = list(tmp['IsWin'].values)\n",
    "    new_cols = []\n",
    "    for game in range(sliding_window_num_of_games):\n",
    "            new_cols.insert(0,'{}_games_back_IsWin'.format(game + 1))\n",
    "    df_of_sequence_of_wins = pd.DataFrame([list_of_sequence_of_wins],columns=new_cols)\n",
    "\n",
    "    # # get team indicator\n",
    "    team_indicator = dummy_curr_team[0:1]\n",
    "    team_indicator\n",
    "\n",
    "    # is at home\n",
    "    list_of_sequence_of_at_home = list(tmp['IsHome'].values)\n",
    "    new_cols = []\n",
    "    for game in range(sliding_window_num_of_games):\n",
    "            new_cols.insert(0,'{}_games_back_IsHome'.format(game + 1))\n",
    "    df_of_sequence_of_at_home = pd.DataFrame([list_of_sequence_of_at_home],columns=new_cols)\n",
    "    df_of_sequence_of_at_home['{}_games_back_IsHome'.format(sliding_window_num_of_games+1)] = is_current_game_at_home\n",
    "\n",
    "\n",
    "    # get opp team indicator \n",
    "\n",
    "    opp_dummy_teams = pd.get_dummies(tmp['Opposing_team_ID'].astype(int).append(pd.Series(opp_team_id)), prefix ='team').reindex(columns = dummy_nba_teams.columns, fill_value=0)\n",
    "    opp_dummy_teams.columns = ['opp_' + col for col in opp_dummy_teams.columns]\n",
    "    appended_list_of_encoded_opp_teams = list(itertools.chain(*opp_dummy_teams.values))\n",
    "    appended_df_of_encoded_opp_teams = pd.DataFrame([appended_list_of_encoded_opp_teams])\n",
    "\n",
    "    # add averages of categories\n",
    "    cols_to_calc = ['PTS','REB','AST','STL','BLK','TOV','PLUS_MINUS','FG3_PCT','FG_PCT','FT_PCT','num_of_possessions',\n",
    "                       'AdjustedPM','OffRating','EFG','AST_ratio','Opp_points','Def_Rating']\n",
    "    averages_of_categories_curr_team = tmp[cols_to_calc].mean().values\n",
    "    df_of_averages_of_categories_curr_team = pd.DataFrame([averages_of_categories_curr_team],columns=cols_to_calc)\n",
    "    df_of_averages_of_categories_curr_team\n",
    "\n",
    "    # add shot area avgs for curr team\n",
    "    shot_index= ['Mid-Range', 'In The Paint (Non-RA)', 'Restricted Area',\n",
    "    'Above the Break 3', 'Left Corner 3', 'Right Corner 3']\n",
    "\n",
    "    shot_chart_df_for_tmp = shot_chart_df[(shot_chart_df['GAME_ID'].isin(tmp['GAME_ID'].values))&\n",
    "                             (shot_chart_df['TEAM_ID']==curr_team_id)].groupby(\n",
    "                                ['SHOT_ZONE_BASIC']).mean()['SHOT_MADE_FLAG'].reset_index()\n",
    "    shot_chart_df_for_tmp_reindex = shot_chart_df_for_tmp[['SHOT_ZONE_BASIC','SHOT_MADE_FLAG']].set_index('SHOT_ZONE_BASIC').reindex(shot_index).fillna(0)\n",
    "    shot_area_avgs = shot_chart_df_for_tmp_reindex.T.reset_index(drop=True)\n",
    "    shot_area_avgs\n",
    "\n",
    "    # add shot chart of opposing teams during window\n",
    "    shot_chart_df_for_tmp = shot_chart_df[(shot_chart_df['GAME_ID'].isin(tmp[:-1]['GAME_ID'].values))&\n",
    "                             (shot_chart_df['TEAM_ID']!=curr_team_id)].groupby(\n",
    "                                ['SHOT_ZONE_BASIC']).mean()['SHOT_MADE_FLAG'].reset_index()\n",
    "    shot_chart_df_for_tmp_reindex = shot_chart_df_for_tmp[['SHOT_ZONE_BASIC','SHOT_MADE_FLAG']].set_index('SHOT_ZONE_BASIC').reindex(shot_index).fillna(0)\n",
    "    shot_area_avgs_opp = shot_chart_df_for_tmp_reindex.T.reset_index(drop=True)\n",
    "    shot_area_avgs_opp\n",
    "\n",
    "      # add averages of categories for past opponents\n",
    "    list_of_opp_ids = list(tmp['Opposing_team_ID'].unique())\n",
    "\n",
    "    x_opp = df_g[(df_g['TEAM_ID'].isin(list_of_opp_ids))&\n",
    "                (df_g['GAME_DATE'] < tmp['GAME_DATE'].max())&\n",
    "                (df_g['GAME_DATE'] >= tmp['GAME_DATE'].min())].sort_values('GAME_DATE').reset_index(drop=True)\n",
    "    tmp_opp = x_opp[-sliding_window_num_of_games:ix]\n",
    "    cols_to_calc = ['PTS','REB','AST','STL','BLK','TOV','PLUS_MINUS','FG3_PCT','FG_PCT','FT_PCT','num_of_possessions',\n",
    "                   'AdjustedPM','OffRating','EFG','AST_ratio','Opp_points','Def_Rating']\n",
    "    averages_of_categories_opp_team = tmp_opp[cols_to_calc].mean().values\n",
    "    df_of_averages_of_categories_opp_team = pd.DataFrame([averages_of_categories_opp_team],columns = cols_to_calc).reset_index(drop=True)\n",
    "\n",
    "\n",
    "    # start merging all together \n",
    "    # 1. wins\n",
    "    # 2. team indicator\n",
    "    # 3. at home indicator\n",
    "    # 4. TOV sequence\n",
    "    # print(len(team_indicator.columns),'team_indicator')\n",
    "    # print(len(df_of_sequence_of_wins.columns),'df_of_sequence_of_wins')\n",
    "    # print(len(df_of_sequence_of_at_home.columns),'df_of_sequence_of_at_home')\n",
    "    # print(len(appended_df_of_encoded_opp_teams.columns),'appended_df_of_encoded_opp_teams')\n",
    "    # print(len(df_of_averages_of_categories_curr_team.columns),'df_of_averages_of_categories_curr_team')\n",
    "    # print(len(shot_area_avgs.columns),'shot_area_avgs')\n",
    "    # print(len(shot_area_avgs_opp.columns),'shot_area_avgs_opp')\n",
    "    df_all_features = team_indicator.merge(df_of_sequence_of_wins , how='inner' , left_index=True , right_index=True)\n",
    "    df_all_features = df_all_features.merge(df_of_sequence_of_at_home, how='inner' , left_index=True , right_index=True)\n",
    "    df_all_features = df_all_features.merge(appended_df_of_encoded_opp_teams, how='inner' , left_index=True , right_index=True)\n",
    "    df_all_features = df_all_features.merge(df_of_averages_of_categories_curr_team, how='inner' , left_index=True , right_index=True)\n",
    "    df_all_features = df_all_features.merge(shot_area_avgs, how='inner' , left_index=True , right_index=True)\n",
    "    df_all_features = df_all_features.merge(shot_area_avgs_opp, how='inner' , left_index=True , right_index=True)\n",
    "    df_all_features = df_all_features.merge(df_of_averages_of_categories_opp_team, how='inner' , left_index=True , right_index=True)\n",
    "\n",
    "#     print(df_all_features.columns)\n",
    "    len(X.columns)\n",
    "\n",
    "    print([curr_team_abv,curr_team_id, opp_team, opp_team_id, best_clf.predict_proba(df_all_features)[0][1]])\n",
    "    print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
